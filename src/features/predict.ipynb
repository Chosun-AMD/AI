{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760ecd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import msgpack\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94e0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_len = 128\n",
    "truncate = lambda x: x[:truncate_len] # String truncation function for the tokenizer (length: 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36048032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer with the vocabulary\n",
    "def load_tokenizer(filename: str) -> Tokenizer:\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"Could not find file {filename}\")\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.word_index = msgpack.load(open(filename, 'rb'))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbef9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences\n",
    "def get_padded_seqence(data, maxlen: int):\n",
    "    return keras.preprocessing.sequence.pad_sequences(data, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72537b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text using the tokenizer\n",
    "def get_padded_seqence_tokenizer(tokenizer: Tokenizer, texts: str, maxlen: int):\n",
    "    _sequence = tokenizer.texts_to_sequences([texts])\n",
    "    return get_padded_seqence(_sequence, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e09e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datas(sha256sum: str, feature_dir: str, ascii_tokenizer: Tokenizer, utf16le_tokenizer: Tokenizer, opcode_tokenizer: Tokenizer):\n",
    "    ascii_path = os.path.join(feature_dir, 'strings', f'{sha256sum}.ascii.msgpack')\n",
    "    utf16le_path = os.path.join(feature_dir, 'strings', f'{sha256sum}.utf16le.msgpack')\n",
    "    opcode_path = os.path.join(feature_dir, 'opcode', f'{sha256sum}.msgpack')\n",
    "    pe_header_path = os.path.join(feature_dir, 'pe_header', f'{sha256sum}.bin')\n",
    "    rich_header_path = os.path.join(feature_dir, 'rich_header', f'{sha256sum}.bin')\n",
    "\n",
    "    # Load data from file\n",
    "    ascii_data = msgpack.load(open(ascii_path, 'rb'))[0]\n",
    "    utf16le_data = msgpack.load(open(utf16le_path, 'rb'))[0]\n",
    "    opcode_data = msgpack.load(open(opcode_path, 'rb'))[0]\n",
    "    pe_header_data = np.fromfile(pe_header_path, dtype=np.uint8)\n",
    "    rich_header_data = np.fromfile(rich_header_path, dtype=np.uint8)\n",
    "\n",
    "    # ASCII Data\n",
    "    # 1. Truncate the data by 128\n",
    "    # 2. Tokenize the data\n",
    "    # 3. Padding the data\n",
    "    ascii_data = ' '.join([truncate(x) for x in ascii_data])\n",
    "    ascii_data = get_padded_seqence_tokenizer(ascii_tokenizer, ascii_data, 100)\n",
    "\n",
    "    # UTF16LE Data\n",
    "    # 1. Truncate the data by 128\n",
    "    # 2. Tokenize the data\n",
    "    # 3. Padding the data\n",
    "    utf16le_data = truncate(utf16le_data)\n",
    "    utf16le_data = get_padded_seqence_tokenizer(utf16le_tokenizer, utf16le_data, 100)\n",
    "\n",
    "    # Opcode Data\n",
    "    # 1. Tokenize the data\n",
    "    # 2. Padding the data\n",
    "    opcode_data = get_padded_seqence_tokenizer(opcode_tokenizer, opcode_data, 100)\n",
    "\n",
    "    # PE Header Data\n",
    "    # 1. Padding the data\n",
    "    # 2. Expand the dimension\n",
    "    pe_header_data = get_padded_seqence([pe_header_data], 4096)\n",
    "    pe_header_data = np.expand_dims(pe_header_data, axis=2)\n",
    "\n",
    "    # Rich Header Data\n",
    "    # 1. Padding the data\n",
    "    # 2. Expand the dimension\n",
    "    rich_header_data = get_padded_seqence([rich_header_data], 512)\n",
    "    rich_header_data = np.expand_dims(rich_header_data, axis=2)\n",
    "    \n",
    "    return ascii_data, utf16le_data, opcode_data, pe_header_data, rich_header_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5616828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model) -> float:\n",
    "    return model.predict(data, verbose=0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ba806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word_index for tokenizer\n",
    "ascii_tokenizer = load_tokenizer('word_index.ascii.msgpack')\n",
    "utf16le_tokenizer = load_tokenizer('word_index.utf16le.msgpack')\n",
    "opcode_tokenizer = load_tokenizer('word_index.opcode.msgpack')\n",
    "\n",
    "# Load Models\n",
    "ascii_model = models.load_model('models/ascii.h5')\n",
    "utf16le_model = models.load_model('models/utf16le.h5')\n",
    "opcode_model = models.load_model('models/opcode.h5')\n",
    "pe_header_model = models.load_model('models/pe_header.h5')\n",
    "rich_header_model = models.load_model('models/rich_header.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e09409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d5dfa906294211a9c295b65411b08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = pd.read_csv('test_set/label.csv', header=None, dtype={0: str, 1: int})\n",
    "csv = csv.rename(columns={0: 'sha256sum', 1: 'label'})\n",
    "result = []\n",
    "for _, row in tqdm(list(csv.iterrows())):\n",
    "    sha256sum = row['sha256sum']\n",
    "    label = row['label']\n",
    "    ascii_data, utf16le_data, opcode_data, pe_header_data, rich_header_data = load_datas(sha256sum, 'test_set', ascii_tokenizer, utf16le_tokenizer, opcode_tokenizer)\n",
    "\n",
    "    result.append(pd.DataFrame({\n",
    "        'sha256sum': sha256sum,\n",
    "        'ascii': predict(ascii_data, ascii_model),\n",
    "        'utf16le': predict(utf16le_data, utf16le_model),\n",
    "        'opcode': predict(opcode_data, opcode_model),\n",
    "        'pe_header': predict(pe_header_data, pe_header_model),\n",
    "        'rich_header': predict(rich_header_data, rich_header_model),\n",
    "        'label': label}, index=[0]))\n",
    "# result = parmap.map(to_df, list(csv.iterrows()), pm_pbar=True, pm_chunksize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1148d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py:403\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    406\u001b[0m         )\n\u001b[1;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "result.to_csv('test_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
